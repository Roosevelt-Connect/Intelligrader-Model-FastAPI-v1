version: '3.8'

services:
  # 1. Your single, user-facing FastAPI Application Service
  fastapi-app:
    # Use the current directory (where Dockerfile is) to build the image
    build: . 
    container_name: fastapi_chat_api
    # This maps the host's port 8080 to the container's port 8000 (as defined in Dockerfile/CMD)
    ports:
      - "8080:8000"
    restart: unless-stopped
    # Include the volume definition if you still need it for logs/data
    volumes:
      # If you keep the volume, you need to define it at the top level (see below)
      # - model_request_data:/app/model_request_data/
      - ./data_logs:/app/model_request_data/ # A simpler, local mount for development

# Define the volume if you still want to use a persistent volume like 'model_request_data'
# volumes:
#   model_request_data:
#     external: trueversion: '3.8'

# Define the volume you created in Portainer
volumes:
  model_request_data:
    external: true

services:
  # 1. The SmollM2 Model Service
  smollm2:
    image: ai/smollm2:latest
    container_name: smollm2_service
    # This service is only accessible internally via the service name 'smollm2'
    # We expose the port internally for the FastAPI app to use
    expose:
      - 8000 
    restart: unless-stopped
    # If the model requires specific environment variables or resources, add them here
    # environment:
    #   - HF_TOKEN=<Your_HuggingFace_Token_if_needed>

  # 2. Your FastAPI Application Service
  fastapi-app:
    build: . # Build from the Dockerfile in the current directory
    container_name: fastapi_chat_api
    depends_on:
      - smollm2 # Ensures the model starts before the chat API
    # Map a host port (e.g., 8080) to the container's port 8000
    ports:
      - "8080:8000" 
    restart: unless-stopped
    volumes:
      # Mount the Portainer volume to the internal log path defined in main.py
      - model_request_data:/app/model_request_data/
